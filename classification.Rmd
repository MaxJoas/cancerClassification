---
title: "Random Forest and SVM for Cancer Classification"
author: "Maximilian Joas"
date: "`r Sys.Date()`"
bibliography: "lib.bib"
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    highlight: zenburn
    self_contained: no
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  ioslides_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    smaller: yes
    toc: yes
    widescreen: yes
  beamer_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    incremental: no
    keep_tex: no
    slide_level: 2
    theme: Montpellier
    toc: yes
  word_document:
    toc: yes
    toc_depth: '3'
  slidy_presentation:
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    incremental: no
    keep_md: yes
    smaller: yes
    theme: cerulean
    toc: yes
    widescreen: yes
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
font-import: http://fonts.googleapis.com/css?family=Risque
subtitle: "Random Forest and SVM for Cancer Classification"
font-family: Garamond
transition: linear
editor_options:
  chunk_output_type: console
---

```{r knitr_settings, include=FALSE, echo=FALSE, eval=TRUE}
library(knitr)
options(width = 300)
knitr::opts_chunk$set(
  fig.width = 7, fig.height = 5,
  fig.path = 'figures/02_combinatorix_',
  fig.align = "center",
  size = "tiny",
  echo = TRUE, eval = TRUE,
  warning = FALSE, message = FALSE,
  results = TRUE, comment = "")


```



## Background
This report is based on a 2009 study by Den Boer et al. The goal of this study was to TODO[[@den2009subtype]]
Genetic subtypes of acute lymphoblastic leukaemia (ALL) are used to determine risk and treatment in children. 25% of precursor B-ALL cases are genetically unclassified and have intermediate prognosis. We aimed to use a genome-wide study to improve prognostic classification of ALL in children. TODO
In the report however I am to use the data set to methodically explore which methods for classification
works best and how different parameters influence the classification. Precisely I use support vector machine (SVM) and random forest for classification. Additionally I use random forest for feature selection.

## Methods Theory
Due to the scope of this work I will not be able to provide a in depth description of the methods.
The aim is to only give a very short overview, so the reader understands the key terms of the methods.

Random forest is a widely used method for classification [@diaz2006gene]. The base of a random forest model
are decision trees. A decision tree consists of multiple nodes. At each node tree is splitted into
branches depending on the value of a independent variable [@es]. In our case this would be the expression level
of a particular gene. Which variable is used at a given split is random. However not all variables are available to sample from at each split. The number of variables available for splitting at each tree node is referred to as the mtry parameter. The end of a branch, the leaf, does not split anymore and represents the decision for a particular class. In our case that would be a cell type.
A random forest consists of multiple decision trees. Each tree gives a vote for the final decision of
the class. The class with the majority vote across all trees is the final class of a random forest.
This number of trees used in a random forest is on interest in this report. I will investigate the
influence of the number of trees on the classification accuracy. One note to parameter tuning


SVM is another well established method for classification it tries to separate the data by a hyperplane.
The goal is that data points in different classes are as far away from the hyperplane as possible. The
data points nearest to the hyperplane are called support vectors [@es]. Often times the data points are not separable in their original dimension. Nevertheless it is always possible to separate the data in a higher dimension [@hofmann2006support]. Therefore we need a way to map the data points to a higher dimension. The function used for this process is called Kernel function. There are different types of Kernels and I will investigate
the influence of the Kernel function on the classification accuracy.
In order to validate the results I implemented leave one out cross validation (see section "Methods Implementation"). The evaluation criteria was the missclassification error (MER) respectively the Accuracy of the predictions. Since I was dealing with a multiclass classification task, looking at specificity and sensitivity is not too interesting. 

In microarray studies it is common to have a high number of measurements in comparison to the number of
patients [@smallnlargep]. This is called a small n large p dataset and can lead to overfitting [@johnstone2009statistical]. Therefore, it be benifical to select only a subset of independent variables. Ideally the ones that contain the most information. In order to find these variables I also used random forest for feature selection. Precisely
I used the impurity variable importance of the random forest. Before growing the random forest for the feature selection I tuned the mtry parameter.

## Methods Implementation
Now that I covered the theoretical basics to understand this report, it is time to present the practical
implementation of the above described methods. In order to test the classification accuracy I need to
split the data into a training data set and a validation data set. I order to have as many training and test patients as possible I decided to use leave one out cross validation as an approach. The standard way to do this would be to use the caret packages. I decided,
however, to implement the functionality myself. The reason for this is that I also wanted to use
the LOOCV approach for feature selection to avoid any bias. With my own implementation I could be sure
that I used the exact same method for feature selection and the training of the model. For my Implementation I used the foreach package for parallelization, which is highly optimized. Thus,
my implementation is also highly performant. The logic of my function is structured as follows:
I have a base function that performs the actual task, e.g feature selection or classification and another function that implements the LOOCV logic that calls the base function in each iteration.
This makes the code easy to extend, since I  only need to write a new base function for a new classification method and can use my existing function for the LOOCV logic.
The base function for classification take the parameters for the corresponding method, e.g. number of trees for random forest and the Kernel method for SVM.

For the feature selection I applied the same principles I had one function that did the actual feature selection and one function that implements the LOOCV logic and calls the feature selection function. I is also possible to specify how many feature should be selected. However I did not want to chose the number of features arbitrary. Consequently, I sorted the variable importance of all covariates and plotted it. Subsequently, I smoothed the plot in order to make it differentiable. The goal was to find the point where the variable importance drops the steepest. This point I used as a cutoff for the number of features that I selected. In order to find the point I took the highest value of the second derivative
of the smoothed line. This process is a highly reuasable ant automated process to find the number of
variables to select.

## Data 


## Results and Interpretation
The result section is structured as follows: Firstly I present the results of the feature selection process. Secondly I will give an overview of the results of the random forest classifier for different number of trees. Subsequently I will present the most meaningful results more in depth. I will do the same for the SVM classifier afterwards. Finally I will compare the best classifiers across the two methods in depth. Each presentation of result is followed by a short interpreation of it.



```{r echo=FALSE}

read_chunk('./classification.R')

```
```{r loading, eval=TRUE}

```
I included only classes that contained more than 30 patients. After the filtering four classes remained. Figure 1 gives an overview of the remaining classes and how many cases each class contains.
```{r filter,eval=TRUE, fig.width=1.5, fig.height=1.5, fig.cap=" Figure 1: Remaining Classes after Class Filtering. T: T-cell acute lymphoblastic leukaemia, Bo: Acute B Lymphoblastic Leukemia, Bt: TEL-AML1 , Bh: hyperdiploid childhood acute lymphoblastic leukemia"}

```
In order to find a number of genes to select I plotted the variable importance of the 200 most important genes according to the random forest variable importance. Figure 2 shows this plot together with a smoothed line of the variable importance and the point where the variable importance drops the steepest( according to the smoothed line).

```{r prepareSelect, eval=TRUE}

```
Accordinlgy to the plot I chose to use 50 genes, when performing the feature selection.
For the comparison of the classifiers I used two sets of selected genes: One set where I selected 50 genes and one set where I included all genes.

```{r select, eval=TRUE}

```

```{r overviewResultsRF, eval=TRUE}

```
 Furthermore I used the following number of trees for the random forest: 200, 500 and 1000. Figure 3 shows an overview of the MER, Accuracy and its 95% confidence interval.
 ```{r execute, eval=TRUE}

```
The accuracy of when using all genes is with 94% very high. The 95% confidence interval is also reasonably small, which gives security about the result
The number of trees seems to have no influence on the prediction accuracy when using all genes.
- suprisingly no difference
- on other hand already a lot of genes
- only difference in feature selection -> not too much sense
- feautre selection not better -> sense, because rf is good at dealing with small n large p (splitting)
- den Boer similar classification

- in depth rf 1000 exemplatory of rf all genes and rf 200 feature selection genes


In the following I wil report on the detailled results of the Random Forest Classifier with 1000 trees and all genes (which yields the exact same results as the other Random Forest Classifiers expect one). Additionally, I report on the results of the Random Forest Classifier when trained with 50 genes and 200 trees.

```{r displayResult,eval=TRUE}

```


TODO epxlain result in depth

The second method of interest in this work was the SVM for classification. Again I start with an overview and discuss more detail later on. For the SVM I used four different kernel methods, namely radial, linear, polynomial and simgmoidal. As done with the Random Forest Classifier I used one time all genes and one time 50 genes, selected via the Random Forest variable Importance, as described above.

```{r overviewResultsSVM, eval=TRUE}
```

TODO explain SVM overview results 
- allgemein
- feature selection
- no feature selection
- kernel influence

```{r displaySVM, eval=TRUE}
```

TODO explain SVM in Depth
"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
```{r inDepth}
```
TODO 
"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
## Conclusion
"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
## References



