<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Maximilian Joas" />

<meta name="date" content="2020-04-20" />

<title>Random Forest and SVM for Cancer Classification</title>

<script src="classification_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="classification_files/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="classification_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="classification_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="classification_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="classification_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="classification_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="classification_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="classification_files/navigation-1.1/tabsets.js"></script>
<script src="classification_files/navigation-1.1/codefolding.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  { color: #cccccc; background-color: #303030; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ffcfaf; } /* Alert */
code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #dca3a3; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #f0dfaf; } /* ControlFlow */
code span.ch { color: #dca3a3; } /* Char */
code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code span.co { color: #7f9f7f; } /* Comment */
code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code span.do { color: #7f9f7f; } /* Documentation */
code span.dt { color: #dfdfbf; } /* DataType */
code span.dv { color: #dcdccc; } /* DecVal */
code span.er { color: #c3bf9f; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #c0bed1; } /* Float */
code span.fu { color: #efef8f; } /* Function */
code span.im { } /* Import */
code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
code span.kw { color: #f0dfaf; } /* Keyword */
code span.op { color: #f0efd0; } /* Operator */
code span.ot { color: #efef8f; } /* Other */
code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code span.sc { color: #dca3a3; } /* SpecialChar */
code span.ss { color: #cc9393; } /* SpecialString */
code span.st { color: #cc9393; } /* String */
code span.va { } /* Variable */
code span.vs { color: #cc9393; } /* VerbatimString */
code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */

.sourceCode .row {
  width: 100%;
}
.sourceCode {
  overflow-x: auto;
}
.code-folding-btn {
  margin-right: -30px;
}
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Random Forest and SVM for Cancer Classification</h1>
<h3 class="subtitle">Random Forest and SVM for Cancer Classification</h3>
<h4 class="author">Maximilian Joas</h4>
<h4 class="date">2020-04-20</h4>

</div>


<div id="background" class="section level1">
<h1>Background</h1>
<p>This report is based on a 2009 study by Den Boer et al. <span class="citation">(Den Boer et al. 2009)</span>. The goal of this study was to improve the prognostic classification of genetic subtypes of acute lymphoblastic leukemia (ALL) in children. The knowledge of these subtypes facilitate risk stratification and can thus be used to choose the appropriate treatment option.</p>
<p>In this report, however, I use the Den Boer data set to methodically explore which methods for classification work best and how different parameters influence the classification. Precisely I use support vector machine (SVM) and random forest for classification. Additionally, I use a random forest for feature selection. The aim of this report is to investigate the influence of the used kernel in the SVM Classifier and the influence of the number of trees in the Random Forest Classifier. Additionally, I investigate the influence of feature selection on these two classifiers.</p>
</div>
<div id="methods-and-data" class="section level1">
<h1>Methods and Data</h1>
<div id="data" class="section level2">
<h2>Data</h2>
<p>The used data consists of a data table that stores information about the gene expression. Gene expression was measured with microarrays. The data has been already pre-processed before I used it. This included: filtering of barely weakly expressed genes, log2 transformation to normalize the raw measurements, intersample standardization. Additionally, I worked with a data table that stored the metadata of the samples belonging to the gene expression values. In our case, the information of interest is the class of the subtype of acute lymphoblastic leukemia. Table 1 shows the frequency of the different subtypes before filtering (filtering will be done in a later step).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">library</span>(gdata)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="kw">library</span>(ranger)</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="kw">library</span>(foreach)</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="kw">library</span>(doMC)</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="kw">library</span>(e1071)</a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="kw">library</span>(pROC)</a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="kw">library</span>(grid)</a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="kw">library</span>(gridExtra)</a>
<a class="sourceLine" id="cb1-11" data-line-number="11"></a>
<a class="sourceLine" id="cb1-12" data-line-number="12"></a>
<a class="sourceLine" id="cb1-13" data-line-number="13"><span class="co">## done in class, loads all the input data</span></a>
<a class="sourceLine" id="cb1-14" data-line-number="14"><span class="kw">load</span>(<span class="st">&quot;./DenBoerData_loaded.Rdata&quot;</span>)</a>
<a class="sourceLine" id="cb1-15" data-line-number="15"></a>
<a class="sourceLine" id="cb1-16" data-line-number="16"><span class="co">## register a paralell backend and define number of Threads</span></a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="co">## Note the the numberThreads variable gets passed later to the ranger function</span></a>
<a class="sourceLine" id="cb1-18" data-line-number="18"><span class="co">## this is neccessary, because ranger uses all available threads as a default and I</span></a>
<a class="sourceLine" id="cb1-19" data-line-number="19"><span class="co">## did not intend to overload the cluster.</span></a>
<a class="sourceLine" id="cb1-20" data-line-number="20">numberThreads &lt;-<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb1-21" data-line-number="21">doMC<span class="op">::</span><span class="kw">registerDoMC</span>(numberThreads)</a>
<a class="sourceLine" id="cb1-22" data-line-number="22"><span class="co">## Here I store all the different parameters we want to use for the different</span></a>
<a class="sourceLine" id="cb1-23" data-line-number="23"><span class="co">## classification methods</span></a>
<a class="sourceLine" id="cb1-24" data-line-number="24">numberOfTrees &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb1-25" data-line-number="25">kernels &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;radial&quot;</span>, <span class="st">&quot;linear&quot;</span>, <span class="st">&quot;polynomial&quot;</span>, <span class="st">&quot;sigmoid&quot;</span>)</a>
<a class="sourceLine" id="cb1-26" data-line-number="26"><span class="co">## for reusability provide the name of the class variable for this dataset</span></a>
<a class="sourceLine" id="cb1-27" data-line-number="27"><span class="co">## I also save the minimum number allowed cases per class in an extra variable</span></a>
<a class="sourceLine" id="cb1-28" data-line-number="28">classVariable &lt;-<span class="st"> &quot;sample.labels&quot;</span></a>
<a class="sourceLine" id="cb1-29" data-line-number="29">minNumberofCasesPerClass &lt;-<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb1-30" data-line-number="30">preFiltered &lt;-<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb1-31" data-line-number="31">parameters &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;RF 200 Trees all Genes&quot;</span>,</a>
<a class="sourceLine" id="cb1-32" data-line-number="32">                <span class="st">&quot;RF 500 Trees all Genes&quot;</span>,</a>
<a class="sourceLine" id="cb1-33" data-line-number="33">                <span class="st">&quot;RF 1000 Trees all Genes&quot;</span>,</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">                <span class="st">&quot;RF 200 Trees Feature Selection&quot;</span>,</a>
<a class="sourceLine" id="cb1-35" data-line-number="35">                <span class="st">&quot;RF 500 Trees Feature Selection&quot;</span>,</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">                <span class="st">&quot;RF 1000 Trees Feature Selection&quot;</span>,</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">                <span class="st">&quot;SVM Radial Kernel all Genes&quot;</span>,</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">                <span class="st">&quot;SVM Linear Kernel all Genes&quot;</span>,</a>
<a class="sourceLine" id="cb1-39" data-line-number="39">                <span class="st">&quot;SVM Ploynomial Kernel all Genes&quot;</span>,</a>
<a class="sourceLine" id="cb1-40" data-line-number="40">                <span class="st">&quot;SVM Sigmoid Kernel all Genes&quot;</span>,</a>
<a class="sourceLine" id="cb1-41" data-line-number="41">                <span class="st">&quot;SVM Radial Kernel Feature Selection&quot;</span>,</a>
<a class="sourceLine" id="cb1-42" data-line-number="42">                <span class="st">&quot;SVM Linear Kernel Feature Selection&quot;</span>,</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">                <span class="st">&quot;SVM Ploynomial Kernel Feature Selection&quot;</span>,</a>
<a class="sourceLine" id="cb1-44" data-line-number="44">                <span class="st">&quot;SVM Sigmoid Kernel Feature Selection&quot;</span>)</a>
<a class="sourceLine" id="cb1-45" data-line-number="45"></a>
<a class="sourceLine" id="cb1-46" data-line-number="46"></a>
<a class="sourceLine" id="cb1-47" data-line-number="47"><span class="co">####  Define Local directories and files ####</span></a>
<a class="sourceLine" id="cb1-48" data-line-number="48"><span class="co">## Find the home directory</span></a>
<a class="sourceLine" id="cb1-49" data-line-number="49">myHome &lt;-<span class="st"> </span><span class="kw">Sys.getenv</span>(<span class="st">&quot;HOME&quot;</span>)</a>
<a class="sourceLine" id="cb1-50" data-line-number="50"></a>
<a class="sourceLine" id="cb1-51" data-line-number="51"><span class="co">## Define the main directory for the  data and results</span></a>
<a class="sourceLine" id="cb1-52" data-line-number="52">mainDir &lt;-<span class="st"> </span><span class="kw">file.path</span>(myHome, <span class="st">&quot;uni/4_sem/biostatistics/report&quot;</span>)</a>
<a class="sourceLine" id="cb1-53" data-line-number="53"><span class="kw">dir.create</span>(<span class="dt">path =</span> mainDir, <span class="dt">showWarnings =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1-54" data-line-number="54"><span class="kw">message</span>(<span class="st">&quot;Main dir: &quot;</span>, mainDir)</a>
<a class="sourceLine" id="cb1-55" data-line-number="55"></a>
<a class="sourceLine" id="cb1-56" data-line-number="56"><span class="co">## Define a file where we wills store the memory image</span></a>
<a class="sourceLine" id="cb1-57" data-line-number="57">memImageFile &lt;-<span class="st"> </span><span class="kw">file.path</span>(mainDir, <span class="st">&quot;DenBoerData_loaded.Rdata&quot;</span>)</a>
<a class="sourceLine" id="cb1-58" data-line-number="58"></a>
<a class="sourceLine" id="cb1-59" data-line-number="59"><span class="co">## Define the dir where we will download the data</span></a>
<a class="sourceLine" id="cb1-60" data-line-number="60">destDir &lt;-<span class="st"> </span><span class="kw">file.path</span>(mainDir, <span class="st">&quot;data&quot;</span>)</a>
<a class="sourceLine" id="cb1-61" data-line-number="61"><span class="kw">message</span>(<span class="st">&quot;Local data dir: &quot;</span>, destDir)</a>
<a class="sourceLine" id="cb1-62" data-line-number="62"></a>
<a class="sourceLine" id="cb1-63" data-line-number="63"><span class="co">## Define a file where we will store all results</span></a>
<a class="sourceLine" id="cb1-64" data-line-number="64">resultDir &lt;-<span class="st"> </span><span class="kw">file.path</span>(mainDir, <span class="st">&quot;resultDir&quot;</span>)</a>
<a class="sourceLine" id="cb1-65" data-line-number="65"><span class="kw">message</span>(<span class="st">&quot;Result direcotry&quot;</span>, resultDir)</a>
<a class="sourceLine" id="cb1-66" data-line-number="66"></a>
<a class="sourceLine" id="cb1-67" data-line-number="67"><span class="co">## check if pheno data and expression data have same order of patients</span></a>
<a class="sourceLine" id="cb1-68" data-line-number="68">check &lt;-<span class="st"> </span><span class="kw">colnames</span>(exprTable) <span class="op">==</span><span class="st"> </span><span class="kw">rownames</span>(phenoTable)</a>
<a class="sourceLine" id="cb1-69" data-line-number="69"><span class="kw">message</span>(<span class="st">&quot;Do Pheno and Expressiondata have the same order of patients&quot;</span>)</a>
<a class="sourceLine" id="cb1-70" data-line-number="70"><span class="kw">message</span>(<span class="kw">all</span>(<span class="dt">check =</span> <span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb1-71" data-line-number="71"></a>
<a class="sourceLine" id="cb1-72" data-line-number="72"><span class="co">## Transposing the exprTable do that it has the right format for the package</span></a>
<a class="sourceLine" id="cb1-73" data-line-number="73">exprTableTransposed &lt;-<span class="st"> </span><span class="kw">t</span>(exprTable)</a>
<a class="sourceLine" id="cb1-74" data-line-number="74"></a>
<a class="sourceLine" id="cb1-75" data-line-number="75"><span class="co">## this prevents an error in the ranger function due to ilegal names</span></a>
<a class="sourceLine" id="cb1-76" data-line-number="76"><span class="kw">colnames</span>(exprTableTransposed) &lt;-<span class="st"> </span><span class="kw">make.names</span>(<span class="kw">colnames</span>(exprTableTransposed))</a></code></pre></div>
</div>
<div id="methods-theory" class="section level2">
<h2>Methods Theory</h2>
<p>Due to the scope of this work, I will not be able to provide an in-depth description of the methods. The aim is to only give a very short overview, so the reader understands the key terms of the methods.</p>
<p>Random forest is a widely used method for classification <span class="citation">(Dı'az-Uriarte and De Andres 2006)</span>. The base of a random forest model are decision trees. A decision tree consists of multiple nodes. At each node the tree is split into branches depending on the value of an independent variable <span class="citation">(Hastie, Tibshirani, and Friedman 2009)</span>. In our case, this would be the expression level of a particular gene. Which variable is used at a given split is random. However, not all variables are available to sample from at each split. The number of variables available for splitting at each tree node is referred to as the mtry parameter. The end of a branch, the leaf, does not split anymore and represents the decision for a particular class. In our case that would be a cell type. A random forest consists of multiple decision trees. Each tree gives a vote for the final decision of the class. The class with the majority vote across all trees is the final class of a random forest. This number of trees used in a random forest is on interest in this report. I will investigate the influence of the number of trees on the classification accuracy. One note to parameter tuning</p>
<p>SVM is another well-established method for classification it tries to separate the data by a hyperplane. The goal is that data points in different classes are as far away from the hyperplane as possible. The data points nearest to the hyperplane are called support vectors <span class="citation">(Hastie, Tibshirani, and Friedman 2009)</span>. Often times the data points are not separable in their original dimension. Nevertheless, it is always possible to separate the data in a higher dimension <span class="citation">(Hofmann 2006)</span>. Therefore we need a way to map the data points to a higher dimension. The function used for this process is called Kernel function. There are different types of Kernels and I will investigate the influence of the Kernel function on the classification accuracy. In order to validate the results, I implemented to leave-one-out-cross -validation (see section “Methods Implementation”). The evaluation criteria were the misclassification error (MER) respectively the Accuracy of the predictions. Since I was dealing with a multiclass classification task, looking at specificity and sensitivity is not too interesting.</p>
<p>In microarray studies, it is common to have a high number of measurements in comparison to the number of patients <span class="citation">(Kosorok, Ma, and others 2007)</span>. This is called a small n large p dataset and can lead to overfitting <span class="citation">(Johnstone and Titterington 2009)</span>. Therefore, it is beneficial to select only a subset of independent variables. Ideally, the ones that contain the most information. In order to find these variables I also used the random forest for feature selection. Precisely, I used the impurity variable importance of the random forest. Before growing the random forest for the feature selection I tuned the mtry parameter.</p>
</div>
<div id="methods-implementation" class="section level2">
<h2>Methods Implementation</h2>
<p>Now that I covered the theoretical basics to understand this report, it is time to present the practical implementation of the above-described methods. In order to test the classification accuracy, I need to split the data into a training data set and a validation data set. I order to have as many training and test patients as possible I decided to use leave one out cross-validation as an approach. The standard way to do this would be to use the caret packages. I decided, however, to implement the functionality myself. The reason for this is that I also wanted to use the LOOCV approach for feature selection to avoid any bias. With my own implementation, I could be sure that I used the exact same method for feature selection and the training of the model. For my Implementation, I used the foreach package for parallelization, which is highly optimized. Thus, my implementation is also highly performant. The logic of my function is structured as follows: I have a base function that performs the actual task, e.g feature selection or classification and another function that implements the LOOCV logic that calls the base function in each iteration. This makes the code easy to extend since I only need to write a new base function for a new classification method and can use my existing function for the LOOCV logic. The base function for classification takes the parameters for the corresponding method, e.g. the number of trees for the random forest and the Kernel method for SVM.</p>
<p>For the feature selection, I applied the same principles I had one function that did the actual feature selection and one function that implements the LOOCV logic and calls the feature selection function. It is also possible to specify how many features should be selected. However, I did not want to choose the number of features arbitrary. Consequently, I sorted the variable importance of all covariates and plotted it. Subsequently, I smoothed the plot in order to make it differentiable. The goal was to find the point where the variable importance drops the steepest. This point I used as a cutoff for the number of features that I selected. In order to find the point, I took the highest value of the second derivative of the smoothed line. This process is a highly reusable and automated process to find the number of variables to select.</p>
</div>
</div>
<div id="results-and-interpretation" class="section level1">
<h1>Results and Interpretation</h1>
<p>The result section is structured as follows: Firstly, I present the results of the feature selection process. Secondly, I will give an overview of the results of the random forest classifier for different numbers of trees. Subsequently, I will present the most meaningful results more in-depth. I will do the same for the SVM classifier afterward. Finally, I will compare the best classifiers across the two methods in depth. Each description of a Table / Figure is followed by a short interpretation.</p>
<p>I included only classes that contained more than 30 patients. After the filtering, four classes remained. Figure 1 gives an overview of the remaining classes and how many cases each class contains.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co">## first we filter the classes and include only classes with more than 30 patients</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="co"># Function for class filtering</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">FilterClass &lt;-<span class="st"> </span><span class="cf">function</span>(minimumNumberofCases){</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">  releventClasses &lt;-<span class="st"> </span><span class="kw">names</span>(<span class="kw">which</span>(<span class="kw">table</span>(</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">    phenoTable[, classVariable]) <span class="op">&gt;</span><span class="st"> </span>minimumNumberofCases))</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">  helper &lt;-<span class="st"> </span>phenoTable[, classVariable] <span class="op">%in%</span><span class="st"> </span>releventClasses</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">  <span class="kw">return</span>(helper)</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb2-9" data-line-number="9"></a>
<a class="sourceLine" id="cb2-10" data-line-number="10">relevantPatients &lt;-<span class="st"> </span><span class="kw">FilterClass</span>(minNumberofCasesPerClass)</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">phenoTable &lt;-<span class="st"> </span>phenoTable[relevantPatients, ]</a>
<a class="sourceLine" id="cb2-12" data-line-number="12"><span class="co">#remove unused factors</span></a>
<a class="sourceLine" id="cb2-13" data-line-number="13">phenoTable[,classVariable] &lt;-<span class="st"> </span><span class="kw">factor</span>(phenoTable[,classVariable])</a>
<a class="sourceLine" id="cb2-14" data-line-number="14">phenoTable<span class="op">$</span>Sample.title &lt;-<span class="st"> </span><span class="kw">factor</span>(phenoTable<span class="op">$</span>Sample.title)</a>
<a class="sourceLine" id="cb2-15" data-line-number="15">exprTableTransposed &lt;-<span class="st"> </span>exprTableTransposed[relevantPatients, ]</a>
<a class="sourceLine" id="cb2-16" data-line-number="16">classesAfterFiltered &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">sort</span>(<span class="kw">table</span>(phenoTable<span class="op">$</span>Sample.title),</a>
<a class="sourceLine" id="cb2-17" data-line-number="17">                                           <span class="dt">decreasing =</span> <span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb2-18" data-line-number="18"><span class="kw">colnames</span>(classesAfterFiltered) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Class&quot;</span>, <span class="st">&quot;Freq&quot;</span>)</a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="kw">kable</span>(classesAfterFiltered, <span class="dt">caption =</span> <span class="st">&quot;Table 2: Class Sizes after Filtering&quot;</span>)</a></code></pre></div>
<table>
<caption>Table 2: Class Sizes after Filtering</caption>
<thead>
<tr class="header">
<th align="left">Class</th>
<th align="right">Freq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">hyperdiploid</td>
<td align="right">44</td>
</tr>
<tr class="even">
<td align="left">pre-B ALL</td>
<td align="right">44</td>
</tr>
<tr class="odd">
<td align="left">TEL-AML1</td>
<td align="right">43</td>
</tr>
<tr class="even">
<td align="left">T-ALL</td>
<td align="right">36</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co">#grid.arrange(tableGrob(as.data.frame(classesAfterFiltered)))</span></a></code></pre></div>
<p>In order to find a number of genes to select I plotted the variable importance of the 200 most important genes according to the random forest variable importance. Figure 2 shows this plot together with a smoothed line of the variable importance and the point where the variable importance drops the steepest (according to the smoothed line).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co">## Function that tunes the mytry parameter of RF</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">tuneMtry &lt;-<span class="st"> </span><span class="cf">function</span>(relevantGenes) {</a>
<a class="sourceLine" id="cb4-4" data-line-number="4">  <span class="co">## first we tune the mtry parameter of the random forest model with caret</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">  <span class="co">## 10 folds repeat 3 times</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6">  control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&#39;repeatedcv&#39;</span>,</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">                          <span class="dt">number =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">                          <span class="dt">repeats =</span> <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb4-9" data-line-number="9">                          <span class="dt">search =</span> <span class="st">&#39;random&#39;</span>)</a>
<a class="sourceLine" id="cb4-10" data-line-number="10">  <span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb4-11" data-line-number="11">  rf.random &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">y =</span> phenoTable[,classVariable],</a>
<a class="sourceLine" id="cb4-12" data-line-number="12">                     <span class="dt">x =</span> exprTableTransposed[, relevantGenes],</a>
<a class="sourceLine" id="cb4-13" data-line-number="13">                     <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb4-14" data-line-number="14">                     <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</a>
<a class="sourceLine" id="cb4-15" data-line-number="15">                     <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb4-16" data-line-number="16"></a>
<a class="sourceLine" id="cb4-17" data-line-number="17">  tunedMtry &lt;-<span class="st"> </span>rf.random<span class="op">$</span>finalModel<span class="op">$</span>mtry</a>
<a class="sourceLine" id="cb4-18" data-line-number="18">  <span class="kw">return</span> (tunedMtry)</a>
<a class="sourceLine" id="cb4-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb4-20" data-line-number="20"></a>
<a class="sourceLine" id="cb4-21" data-line-number="21"></a>
<a class="sourceLine" id="cb4-22" data-line-number="22">tunedMtryAllFeatures &lt;-<span class="st"> </span><span class="kw">tuneMtry</span>(<span class="kw">colnames</span>(exprTableTransposed))</a>
<a class="sourceLine" id="cb4-23" data-line-number="23">tunedMtry &lt;-<span class="st"> </span>tunedMtryAllFeatures</a>
<a class="sourceLine" id="cb4-24" data-line-number="24"><span class="co">## Function that selects genes based on the random forest variable importance</span></a>
<a class="sourceLine" id="cb4-25" data-line-number="25"><span class="co">## It takes a vector as index variable that indicates which patients should be used</span></a>
<a class="sourceLine" id="cb4-26" data-line-number="26">SelectFeaturesWithRandomForest &lt;-<span class="st"> </span><span class="cf">function</span>(trainIndex,</a>
<a class="sourceLine" id="cb4-27" data-line-number="27">                                           numFeatures,</a>
<a class="sourceLine" id="cb4-28" data-line-number="28">                                           <span class="dt">verbose =</span> <span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb4-29" data-line-number="29">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Fitting a Random Forest for feature selection&quot;</span>)</a>
<a class="sourceLine" id="cb4-30" data-line-number="30">  fit &lt;-<span class="st"> </span><span class="kw">ranger</span>(<span class="dt">y =</span> phenoTable[trainIndex, classVariable],</a>
<a class="sourceLine" id="cb4-31" data-line-number="31">                <span class="dt">x =</span> exprTableTransposed[trainIndex,],</a>
<a class="sourceLine" id="cb4-32" data-line-number="32">                <span class="dt">importance =</span> <span class="st">&quot;impurity&quot;</span>,</a>
<a class="sourceLine" id="cb4-33" data-line-number="33">                <span class="dt">mtry =</span> tunedMtry,</a>
<a class="sourceLine" id="cb4-34" data-line-number="34">                <span class="dt">num.threads =</span> numberThreads,</a>
<a class="sourceLine" id="cb4-35" data-line-number="35">  )</a>
<a class="sourceLine" id="cb4-36" data-line-number="36"></a>
<a class="sourceLine" id="cb4-37" data-line-number="37">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Finished fitting, now extracting variable importance&quot;</span>)</a>
<a class="sourceLine" id="cb4-38" data-line-number="38">  varImportance &lt;-<span class="st"> </span>fit<span class="op">$</span>variable.importance</a>
<a class="sourceLine" id="cb4-39" data-line-number="39">  selectedGenes &lt;-<span class="st"> </span><span class="kw">sort</span>(varImportance, <span class="dt">na.last =</span> <span class="ot">TRUE</span>, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb4-40" data-line-number="40">  <span class="kw">return</span>(selectedGenes[<span class="dv">1</span><span class="op">:</span>numFeatures])</a>
<a class="sourceLine" id="cb4-41" data-line-number="41">}</a>
<a class="sourceLine" id="cb4-42" data-line-number="42"><span class="co">## get number of features</span></a>
<a class="sourceLine" id="cb4-43" data-line-number="43">getNumberofFeatures &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb4-44" data-line-number="44">  selectedGenes &lt;-<span class="st"> </span><span class="kw">SelectFeaturesWithRandomForest</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(exprTableTransposed)),</a>
<a class="sourceLine" id="cb4-45" data-line-number="45">                                                    <span class="kw">ncol</span>(exprTableTransposed))</a>
<a class="sourceLine" id="cb4-46" data-line-number="46"></a>
<a class="sourceLine" id="cb4-47" data-line-number="47">  lo &lt;-<span class="st"> </span><span class="kw">loess</span>(selectedGenes[<span class="dv">1</span><span class="op">:</span>preFiltered] <span class="op">~</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">200</span>))</a>
<a class="sourceLine" id="cb4-48" data-line-number="48">  smoothed =<span class="st"> </span><span class="kw">predict</span>(lo)</a>
<a class="sourceLine" id="cb4-49" data-line-number="49">  secondDer &lt;-<span class="st"> </span><span class="kw">diff</span>(<span class="kw">diff</span>(smoothed))</a>
<a class="sourceLine" id="cb4-50" data-line-number="50">  maximalChangePoint &lt;-<span class="st"> </span><span class="kw">max</span>(secondDer)</a>
<a class="sourceLine" id="cb4-51" data-line-number="51">  maximalChangeIndex &lt;-<span class="st"> </span><span class="kw">match</span>(maximalChangePoint, secondDer)</a>
<a class="sourceLine" id="cb4-52" data-line-number="52">  numberFeatures &lt;-<span class="st"> </span>maximalChangeIndex</a>
<a class="sourceLine" id="cb4-53" data-line-number="53">  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">numberFeatures =</span> numberFeatures,</a>
<a class="sourceLine" id="cb4-54" data-line-number="54">           <span class="dt">smoothedLine =</span> smoothed,</a>
<a class="sourceLine" id="cb4-55" data-line-number="55">           <span class="dt">genes =</span> selectedGenes,</a>
<a class="sourceLine" id="cb4-56" data-line-number="56">           <span class="dt">maximalChangeIndex =</span> maximalChangeIndex</a>
<a class="sourceLine" id="cb4-57" data-line-number="57">           ))</a>
<a class="sourceLine" id="cb4-58" data-line-number="58">}</a>
<a class="sourceLine" id="cb4-59" data-line-number="59"></a>
<a class="sourceLine" id="cb4-60" data-line-number="60"></a>
<a class="sourceLine" id="cb4-61" data-line-number="61">helper &lt;-<span class="st"> </span><span class="kw">getNumberofFeatures</span>()</a>
<a class="sourceLine" id="cb4-62" data-line-number="62">numberFeatures &lt;-<span class="st"> </span>helper[[<span class="st">&quot;numberFeatures&quot;</span>]]</a>
<a class="sourceLine" id="cb4-63" data-line-number="63"></a>
<a class="sourceLine" id="cb4-64" data-line-number="64">plotVariableImportance &lt;-<span class="st"> </span><span class="cf">function</span>(smoothed,</a>
<a class="sourceLine" id="cb4-65" data-line-number="65">                                   selectedGenes,</a>
<a class="sourceLine" id="cb4-66" data-line-number="66">                                   maximalChangeIndex) {</a>
<a class="sourceLine" id="cb4-67" data-line-number="67">  plotData &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">`</span><span class="dt">Variable Importance</span><span class="st">`</span> =<span class="st"> </span>selectedGenes[<span class="dv">1</span><span class="op">:</span>preFiltered],</a>
<a class="sourceLine" id="cb4-68" data-line-number="68">                         <span class="dt">Index =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>preFiltered),</a>
<a class="sourceLine" id="cb4-69" data-line-number="69">                         <span class="dt">Smoothed =</span> smoothed,</a>
<a class="sourceLine" id="cb4-70" data-line-number="70">                         <span class="dt">check.names =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-71" data-line-number="71">  colors &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal Width&quot;</span> =<span class="st"> &quot;blue&quot;</span>, <span class="st">&quot;Petal Length&quot;</span> =<span class="st"> &quot;red&quot;</span>,</a>
<a class="sourceLine" id="cb4-72" data-line-number="72">              <span class="st">&quot;Petal Width&quot;</span> =<span class="st"> &quot;orange&quot;</span>)</a>
<a class="sourceLine" id="cb4-73" data-line-number="73">  <span class="kw">ggplot</span>(plotData,</a>
<a class="sourceLine" id="cb4-74" data-line-number="74">         <span class="kw">aes</span>(<span class="dt">x =</span> Index)) <span class="op">+</span></a>
<a class="sourceLine" id="cb4-75" data-line-number="75"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="st">`</span><span class="dt">Variable Importance</span><span class="st">`</span>, <span class="dt">color =</span> <span class="st">&quot;Empiric&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb4-76" data-line-number="76"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> smoothed, <span class="dt">color =</span> <span class="st">&quot;smoothed&quot;</span>))  <span class="op">+</span></a>
<a class="sourceLine" id="cb4-77" data-line-number="77"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> maximalChangeIndex, <span class="dt">color=</span> <span class="st">&quot;Turning Point&quot;</span>),</a>
<a class="sourceLine" id="cb4-78" data-line-number="78">             <span class="dt">size =</span> <span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb4-79" data-line-number="79">}</a>
<a class="sourceLine" id="cb4-80" data-line-number="80"></a>
<a class="sourceLine" id="cb4-81" data-line-number="81"></a>
<a class="sourceLine" id="cb4-82" data-line-number="82"><span class="kw">plotVariableImportance</span>(helper[[<span class="st">&quot;smoothedLine&quot;</span>]],</a>
<a class="sourceLine" id="cb4-83" data-line-number="83">                       helper[[<span class="st">&quot;genes&quot;</span>]],</a>
<a class="sourceLine" id="cb4-84" data-line-number="84">                       helper[[<span class="st">&quot;maximalChangeIndex&quot;</span>]])</a></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/02_combinatorix_prepareSelect-1.png" alt="Figure 2: Plot of the development of the Random Forest variable importance of the 200 most important genes. The red dots show the actual value of the variable importance, the green line smoothes these values and the blue vertical line indicated the point where the smoothed line changes the most." width="672" />
<p class="caption">
Figure 2: Plot of the development of the Random Forest variable importance of the 200 most important genes. The red dots show the actual value of the variable importance, the green line smoothes these values and the blue vertical line indicated the point where the smoothed line changes the most.
</p>
</div>
<p>Accordingly to the plot, I chose to use 50 genes, when performing the feature selection. For the comparison of the classifiers, I used two sets of selected genes: One set where I selected 50 genes and one set where I included all genes. Of course, there are many other ways to determine how many features should be included. The advantage of my approach is that it is reusable and that it quantifies the loss of importance of the features. The drawbacks are that it does not account for the total number of features in the data set and the selected number can be unfavorable in extreme cases of the distribution of the variable importance i.e. it selects only one or all genes. However, it was not the focus of this report to find a method that optimally selects the number of features. Thus, I wanted just an automated, out of the box way to do this.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co">#### Feature Selection ####</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="co">## Function that selects genes based on the random forest variable importance</span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3"><span class="co">## It takes a vector as index variable that indicates which patients should be used</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">SelectFeaturesWithRandomForest &lt;-<span class="st"> </span><span class="cf">function</span>(trainIndex,</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">                                           numFeatures,</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">                                           <span class="dt">verbose =</span> <span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Fitting a Random Forest for feature selection&quot;</span>)</a>
<a class="sourceLine" id="cb5-8" data-line-number="8">  fit &lt;-<span class="st"> </span><span class="kw">ranger</span>(<span class="dt">y =</span> phenoTable[trainIndex, classVariable],</a>
<a class="sourceLine" id="cb5-9" data-line-number="9">                <span class="dt">x =</span> exprTableTransposed[trainIndex,],</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">                <span class="dt">importance =</span> <span class="st">&quot;impurity&quot;</span>,</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">                <span class="dt">mtry =</span> tunedMtry,</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">                <span class="dt">num.threads =</span> numberThreads,</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">  )</a>
<a class="sourceLine" id="cb5-14" data-line-number="14"></a>
<a class="sourceLine" id="cb5-15" data-line-number="15">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Finished fitting, now extracting variable importance&quot;</span>)</a>
<a class="sourceLine" id="cb5-16" data-line-number="16">  varImportance &lt;-<span class="st"> </span>fit<span class="op">$</span>variable.importance</a>
<a class="sourceLine" id="cb5-17" data-line-number="17">  selectedGenes &lt;-<span class="st"> </span><span class="kw">sort</span>(varImportance, <span class="dt">na.last =</span> <span class="ot">TRUE</span>, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb5-18" data-line-number="18">  <span class="kw">return</span>(selectedGenes[<span class="dv">1</span><span class="op">:</span>numFeatures])</a>
<a class="sourceLine" id="cb5-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb5-20" data-line-number="20"></a>
<a class="sourceLine" id="cb5-21" data-line-number="21"></a>
<a class="sourceLine" id="cb5-22" data-line-number="22"></a>
<a class="sourceLine" id="cb5-23" data-line-number="23"><span class="co">## Selection with leave one out cross validation</span></a>
<a class="sourceLine" id="cb5-24" data-line-number="24">SelectRandomForestLOOCV &lt;-<span class="st"> </span><span class="cf">function</span>(numFeatures,</a>
<a class="sourceLine" id="cb5-25" data-line-number="25">                                    <span class="dt">verbose =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb5-26" data-line-number="26">  res &lt;-<span class="st"> </span><span class="kw">foreach</span>(<span class="dt">i =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(exprTableTransposed)) <span class="op">%dopar%</span><span class="st"> </span>{</a>
<a class="sourceLine" id="cb5-27" data-line-number="27">    curVariables &lt;-<span class="st"> </span><span class="kw">SelectFeaturesWithRandomForest</span>(<span class="op">-</span>i,</a>
<a class="sourceLine" id="cb5-28" data-line-number="28">                                                   numFeatures,</a>
<a class="sourceLine" id="cb5-29" data-line-number="29">                                                   verbose)</a>
<a class="sourceLine" id="cb5-30" data-line-number="30">    <span class="kw">return</span>(curVariables)</a>
<a class="sourceLine" id="cb5-31" data-line-number="31"></a>
<a class="sourceLine" id="cb5-32" data-line-number="32">  }</a>
<a class="sourceLine" id="cb5-33" data-line-number="33">  <span class="kw">names</span>(res) &lt;-<span class="st"> </span><span class="kw">rownames</span>(phenoTable)</a>
<a class="sourceLine" id="cb5-34" data-line-number="34">  <span class="kw">return</span>(res)</a>
<a class="sourceLine" id="cb5-35" data-line-number="35">}</a>
<a class="sourceLine" id="cb5-36" data-line-number="36"></a>
<a class="sourceLine" id="cb5-37" data-line-number="37">loocvSelections &lt;-<span class="st"> </span><span class="kw">SelectRandomForestLOOCV</span>(numberFeatures)</a>
<a class="sourceLine" id="cb5-38" data-line-number="38"><span class="kw">write.csv</span>(loocvSelections, <span class="st">&#39;loocvSelections.csv&#39;</span>)</a></code></pre></div>
<p>For the actual comparison of the Random Forest Classifiers I used the following numbers of trees: 200, 500 and 1000. Table 3 shows an overview of the MER, Accuracy and the 95% confidence interval.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">getResultOverview &lt;-<span class="st"> </span><span class="cf">function</span> (results) {</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">  evaluationResults &lt;-<span class="st"> </span><span class="kw">foreach</span>(<span class="dt">i =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(results)), <span class="dt">.combine =</span> <span class="st">&#39;rbind&#39;</span>) <span class="op">%do%</span><span class="st"> </span>{</a>
<a class="sourceLine" id="cb6-3" data-line-number="3">    res &lt;-<span class="st"> </span>results[[i]]</a>
<a class="sourceLine" id="cb6-4" data-line-number="4">    cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(res, response)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5">    check &lt;-<span class="st"> </span><span class="kw">names</span>(response) <span class="op">==</span><span class="st"> </span><span class="kw">names</span>(res)</a>
<a class="sourceLine" id="cb6-6" data-line-number="6">    <span class="kw">message</span>(<span class="st">&quot;Patients in Same Order:&quot;</span>)</a>
<a class="sourceLine" id="cb6-7" data-line-number="7">    <span class="kw">message</span>(<span class="kw">all</span>(check,<span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb6-8" data-line-number="8">    cm.table &lt;-<span class="st"> </span>cm<span class="op">$</span>table</a>
<a class="sourceLine" id="cb6-9" data-line-number="9">    hits &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(cm.table))</a>
<a class="sourceLine" id="cb6-10" data-line-number="10">    errors &lt;-<span class="st"> </span><span class="kw">sum</span>(cm.table) <span class="op">-</span><span class="st"> </span>hits</a>
<a class="sourceLine" id="cb6-11" data-line-number="11">    misclError &lt;-<span class="st">  </span>errors <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(cm.table)</a>
<a class="sourceLine" id="cb6-12" data-line-number="12">    accuracy &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>misclError</a>
<a class="sourceLine" id="cb6-13" data-line-number="13">    ci.upper &lt;-<span class="st"> </span>accuracy <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span></a>
<a class="sourceLine" id="cb6-14" data-line-number="14"><span class="st">                </span><span class="kw">sqrt</span>( (accuracy <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>accuracy)) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(exprTableTransposed))</a>
<a class="sourceLine" id="cb6-15" data-line-number="15">    ci.lower &lt;-<span class="st"> </span>accuracy <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span></a>
<a class="sourceLine" id="cb6-16" data-line-number="16"><span class="st">                </span><span class="kw">sqrt</span>( (accuracy <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>accuracy)) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(exprTableTransposed))</a>
<a class="sourceLine" id="cb6-17" data-line-number="17">    ci.interval &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">round</span>(ci.lower,<span class="dv">4</span>), <span class="st">&quot;-&quot;</span>, <span class="kw">round</span>(ci.upper,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb6-18" data-line-number="18">    <span class="kw">message</span>(<span class="st">&quot;Misclassifiction Error for current predictions:&quot;</span>)</a>
<a class="sourceLine" id="cb6-19" data-line-number="19">    <span class="kw">message</span>(<span class="kw">round</span>(misclError,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb6-20" data-line-number="20">    <span class="kw">return</span>(<span class="kw">data.frame</span>(<span class="dt">MER =</span> <span class="kw">round</span>(misclError,<span class="dv">4</span>),</a>
<a class="sourceLine" id="cb6-21" data-line-number="21">                      <span class="dt">Accuracy =</span> <span class="kw">round</span>(accuracy,<span class="dv">4</span>),</a>
<a class="sourceLine" id="cb6-22" data-line-number="22">                      <span class="st">`</span><span class="dt">95% CI</span><span class="st">`</span> =<span class="st"> </span>ci.interval,</a>
<a class="sourceLine" id="cb6-23" data-line-number="23">                      <span class="dt">check.names =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb6-24" data-line-number="24">  }</a>
<a class="sourceLine" id="cb6-25" data-line-number="25">  <span class="kw">rownames</span>(evaluationResults) &lt;-<span class="st"> </span><span class="kw">names</span>(results)</a>
<a class="sourceLine" id="cb6-26" data-line-number="26">  <span class="kw">return</span>(evaluationResults)</a>
<a class="sourceLine" id="cb6-27" data-line-number="27">}</a>
<a class="sourceLine" id="cb6-28" data-line-number="28"></a>
<a class="sourceLine" id="cb6-29" data-line-number="29"></a>
<a class="sourceLine" id="cb6-30" data-line-number="30">resultTable &lt;-<span class="st"> </span><span class="kw">getResultOverview</span>(resultVector)</a>
<a class="sourceLine" id="cb6-31" data-line-number="31">rfResultTable &lt;-<span class="st"> </span>resultTable[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, ]</a>
<a class="sourceLine" id="cb6-32" data-line-number="32"><span class="kw">kable</span>(rfResultTable, <span class="dt">caption =</span> <span class="st">&quot;Table 3: Overview of the Random Forest Classifier Performance with different Parameters&quot;</span>)</a></code></pre></div>
<table>
<caption>Table 3: Overview of the Random Forest Classifier Performance with different Parameters</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">MER</th>
<th align="right">Accuracy</th>
<th align="left">95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RF 200 Trees all Genes</td>
<td align="right">0.0599</td>
<td align="right">0.9401</td>
<td align="left">0.9041-0.9761</td>
</tr>
<tr class="even">
<td>RF 500 Trees all Genes</td>
<td align="right">0.0599</td>
<td align="right">0.9401</td>
<td align="left">0.9041-0.9761</td>
</tr>
<tr class="odd">
<td>RF 1000 Trees all Genes</td>
<td align="right">0.0599</td>
<td align="right">0.9401</td>
<td align="left">0.9041-0.9761</td>
</tr>
<tr class="even">
<td>RF 200 Trees Feature Selection</td>
<td align="right">0.1617</td>
<td align="right">0.8383</td>
<td align="left">0.7825-0.8942</td>
</tr>
<tr class="odd">
<td>RF 500 Trees Feature Selection</td>
<td align="right">0.0599</td>
<td align="right">0.9401</td>
<td align="left">0.9041-0.9761</td>
</tr>
<tr class="even">
<td>RF 1000 Trees Feature Selection</td>
<td align="right">0.0599</td>
<td align="right">0.9401</td>
<td align="left">0.9041-0.9761</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co">#### Classifier Functions ####</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="co">## Function that classifies patients with random forest</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3">RandomForestClassifier &lt;-<span class="st"> </span><span class="cf">function</span>(numberTrees,</a>
<a class="sourceLine" id="cb7-4" data-line-number="4">                                   testIndex,</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">                                   trainIndex,</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">                                   selectedCovariates,</a>
<a class="sourceLine" id="cb7-7" data-line-number="7">                                   <span class="dt">verbose =</span> <span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb7-8" data-line-number="8">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Fitting the Random Forest&quot;</span>)</a>
<a class="sourceLine" id="cb7-9" data-line-number="9">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="kw">paste0</span>(<span class="st">&quot;Using &quot;</span>,numberTrees,<span class="st">&quot; trees&quot;</span>))</a>
<a class="sourceLine" id="cb7-10" data-line-number="10">  rf.fit &lt;-<span class="st"> </span><span class="kw">ranger</span>(<span class="dt">y =</span> phenoTable[trainIndex, classVariable],</a>
<a class="sourceLine" id="cb7-11" data-line-number="11">                   <span class="dt">x =</span> exprTableTransposed[trainIndex, selectedCovariates],</a>
<a class="sourceLine" id="cb7-12" data-line-number="12">                   <span class="dt">num.trees =</span> numberTrees,</a>
<a class="sourceLine" id="cb7-13" data-line-number="13">                   <span class="dt">num.threads =</span> numberThreads,</a>
<a class="sourceLine" id="cb7-14" data-line-number="14">                   <span class="dt">mtry =</span> tunedMtry)</a>
<a class="sourceLine" id="cb7-15" data-line-number="15"></a>
<a class="sourceLine" id="cb7-16" data-line-number="16">  testData &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">as.data.frame</span>(exprTableTransposed[testIndex,</a>
<a class="sourceLine" id="cb7-17" data-line-number="17">                                                  selectedCovariates]))</a>
<a class="sourceLine" id="cb7-18" data-line-number="18">  <span class="cf">if</span>(<span class="kw">length</span>(testIndex) <span class="op">!=</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb7-19" data-line-number="19">    testData &lt;-<span class="st"> </span>exprTableTransposed[testIndex, selectedCovariates]</a>
<a class="sourceLine" id="cb7-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb7-21" data-line-number="21">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Starting prediction based on the fitted model&quot;</span>)</a>
<a class="sourceLine" id="cb7-22" data-line-number="22">  predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(rf.fit, testData)</a>
<a class="sourceLine" id="cb7-23" data-line-number="23">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Finished predicting, now returning the predictions&quot;</span>)</a>
<a class="sourceLine" id="cb7-24" data-line-number="24">  predicted<span class="op">$</span>predictions</a>
<a class="sourceLine" id="cb7-25" data-line-number="25">  <span class="kw">return</span>(predicted<span class="op">$</span>predictions)</a>
<a class="sourceLine" id="cb7-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb7-27" data-line-number="27"></a>
<a class="sourceLine" id="cb7-28" data-line-number="28"></a>
<a class="sourceLine" id="cb7-29" data-line-number="29"></a>
<a class="sourceLine" id="cb7-30" data-line-number="30"></a>
<a class="sourceLine" id="cb7-31" data-line-number="31"></a>
<a class="sourceLine" id="cb7-32" data-line-number="32"><span class="co">## Function that classifies patients with SVM</span></a>
<a class="sourceLine" id="cb7-33" data-line-number="33">SvmClassifier &lt;-<span class="st"> </span><span class="cf">function</span>(myKernel,</a>
<a class="sourceLine" id="cb7-34" data-line-number="34">                          testIndex,</a>
<a class="sourceLine" id="cb7-35" data-line-number="35">                          trainIndex,</a>
<a class="sourceLine" id="cb7-36" data-line-number="36">                          selectedCovariates,</a>
<a class="sourceLine" id="cb7-37" data-line-number="37">                          <span class="dt">verbose =</span> <span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb7-38" data-line-number="38">  <span class="cf">if</span>(verbose) <span class="kw">message</span>(<span class="st">&quot;Starting fitting a SVM Mode&quot;</span>)</a>
<a class="sourceLine" id="cb7-39" data-line-number="39">  svm.fit &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="dt">y =</span> phenoTable[trainIndex, classVariable],</a>
<a class="sourceLine" id="cb7-40" data-line-number="40">                 <span class="dt">x =</span> exprTableTransposed[trainIndex, selectedCovariates],</a>
<a class="sourceLine" id="cb7-41" data-line-number="41">                 <span class="dt">kernel =</span> myKernel,</a>
<a class="sourceLine" id="cb7-42" data-line-number="42">                 <span class="dt">gamma =</span> <span class="fl">0.1</span>,</a>
<a class="sourceLine" id="cb7-43" data-line-number="43">                 <span class="dt">cost =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb7-44" data-line-number="44">                 <span class="dt">type =</span> <span class="st">&quot;C-classification&quot;</span>)</a>
<a class="sourceLine" id="cb7-45" data-line-number="45">  <span class="co"># neccessary in case the test index has length one (loocv)</span></a>
<a class="sourceLine" id="cb7-46" data-line-number="46">  testData &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">as.data.frame</span>(exprTableTransposed[testIndex,</a>
<a class="sourceLine" id="cb7-47" data-line-number="47">                                                  selectedCovariates]))</a>
<a class="sourceLine" id="cb7-48" data-line-number="48">  <span class="cf">if</span>(<span class="kw">length</span>(testIndex) <span class="op">!=</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb7-49" data-line-number="49">    testData &lt;-<span class="st"> </span>exprTableTransposed[testIndex, selectedCovariates]</a>
<a class="sourceLine" id="cb7-50" data-line-number="50">  }</a>
<a class="sourceLine" id="cb7-51" data-line-number="51">  predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(svm.fit, <span class="dt">newdata =</span> testData)</a>
<a class="sourceLine" id="cb7-52" data-line-number="52">  <span class="kw">return</span>(predicted)</a>
<a class="sourceLine" id="cb7-53" data-line-number="53">}</a>
<a class="sourceLine" id="cb7-54" data-line-number="54"></a>
<a class="sourceLine" id="cb7-55" data-line-number="55"></a>
<a class="sourceLine" id="cb7-56" data-line-number="56"></a>
<a class="sourceLine" id="cb7-57" data-line-number="57"><span class="co">#### Function for LOOCV ####</span></a></code></pre></div>
<p>The accuracy when using all genes is with 94% very high. The 95% confidence interval is also reasonably small, which gives security about the result The number of trees seems to have no influence on the prediction accuracy when using all genes. Only when training the Classifier with a subset of selected genes the number of trees seems to influence the prediction accuracy. Increasing the number of trees from 200 to 500, lead to an improvement of the accuracy of 10.6 percentage points. A further increase in the number of trees did not improve the prediction accuracy. The feature selection did not have an impact on the prediction when the Random Forest Classifier was trained with more than 200 trees and it decreased the accuracy when trained with 200 trees.</p>
<p>It is a bit unexpected that the number of trees did not have a meaningful influence on prediction accuracy. The used numbers of trees were all rather a and thud could be already considered as nearly optimal. The reason why I used many trees, is that the data set does contain over 20,000 genes. So using fewer trees could have lead to more differences in the prediction accuracies. The fact that feature selection did not have a meaningful impact on the Classifier is not unexpected. Firstly, Random Forests can deal with a large number of covariates <span class="citation">(Breiman 2001)</span>. This is due to the fact that it uses only a subset of covariates at each split. Secondly, I have already used a Random Forest to select covariates. I will go into more detail about the practical meaning of MER and accuracy at the end of the section when comparing the two best Classifiers of this report.</p>
<p>In the following, I will report on the detailed results of the Random Forest Classifier with 1000 trees and all genes (which yields the exact same results as the other Random Forest Classifiers expect one). Additionally, I report on the results of the Random Forest Classifier when trained with 50 genes and 200 trees.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co">#### Functions for displaying the results ####</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="co">## Functions for plotting a heatmap of a confusion table</span></a>
<a class="sourceLine" id="cb8-3" data-line-number="3">plotResults &lt;-<span class="st"> </span><span class="cf">function</span>(res, response, title) {</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">  cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(res, response)</a>
<a class="sourceLine" id="cb8-5" data-line-number="5">  cm.table &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(cm<span class="op">$</span>table)</a>
<a class="sourceLine" id="cb8-6" data-line-number="6">  cm.stats &lt;-<span class="kw">data.frame</span>(<span class="dt">Stats =</span> cm<span class="op">$</span>overall)</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">  cm.stats<span class="op">$</span>Stats &lt;-<span class="st"> </span><span class="kw">round</span>(cm.stats<span class="op">$</span>Stats,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb8-8" data-line-number="8">  cm.percentage &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">prop.table</span>(cm<span class="op">$</span>table))</a>
<a class="sourceLine" id="cb8-9" data-line-number="9">  cm.table<span class="op">$</span>Perc &lt;-<span class="st"> </span><span class="kw">round</span>(cm.percentage<span class="op">$</span>Freq<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb8-10" data-line-number="10"></a>
<a class="sourceLine" id="cb8-11" data-line-number="11">  cm.plot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> cm.table, <span class="kw">aes</span>(<span class="dt">x =</span> Prediction , <span class="dt">y =</span>  Reference, <span class="dt">fill =</span> Freq)) <span class="op">+</span></a>
<a class="sourceLine" id="cb8-12" data-line-number="12"><span class="st">             </span><span class="kw">geom_raster</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> Freq)) <span class="op">+</span></a>
<a class="sourceLine" id="cb8-13" data-line-number="13"><span class="st">             </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> <span class="kw">paste</span>(<span class="st">&quot;&quot;</span>,Freq,<span class="st">&quot;,&quot;</span>,Perc,<span class="st">&quot;%&quot;</span>)),</a>
<a class="sourceLine" id="cb8-14" data-line-number="14">             <span class="dt">color =</span> <span class="st">&#39;white&#39;</span>, <span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">fontface =</span> <span class="st">&quot;bold&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb8-15" data-line-number="15"><span class="st">             </span><span class="kw">theme_light</span>()</a>
<a class="sourceLine" id="cb8-16" data-line-number="16"></a>
<a class="sourceLine" id="cb8-17" data-line-number="17">  cm.statsTable &lt;-<span class="st">  </span><span class="kw">tableGrob</span>(cm.stats)</a>
<a class="sourceLine" id="cb8-18" data-line-number="18"></a>
<a class="sourceLine" id="cb8-19" data-line-number="19">  <span class="kw">grid.arrange</span>(cm.plot, cm.statsTable,<span class="dt">nrow =</span> <span class="dv">1</span>, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">widths=</span><span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.3</span>))</a>
<a class="sourceLine" id="cb8-20" data-line-number="20">  <span class="co">#top = textGrob(paste0(&quot;Confusion Table Heatmap \n&quot;,title), gp = gpar(fontsize=20,font=1)))</span></a>
<a class="sourceLine" id="cb8-21" data-line-number="21"></a>
<a class="sourceLine" id="cb8-22" data-line-number="22">}</a>
<a class="sourceLine" id="cb8-23" data-line-number="23"></a>
<a class="sourceLine" id="cb8-24" data-line-number="24"></a>
<a class="sourceLine" id="cb8-25" data-line-number="25"><span class="kw">plotResults</span>(resultVector[[<span class="dv">3</span>]], response, <span class="kw">names</span>(resultVector)[<span class="dv">3</span>])</a></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/02_combinatorix_displayResult-1.png" alt="**Figure 1** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the Random Forest Classifier when trained with 1000 trees and all genes. Notes: T: T-ALL, Bt: TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid." width="672" />
<p class="caption">
<strong>Figure 1</strong> shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the Random Forest Classifier when trained with 1000 trees and all genes. Notes: T: T-ALL, Bt: TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid.
</p>
</div>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">plotResults</span>(resultVector[[<span class="dv">4</span>]], response, <span class="kw">names</span>(resultVector)[<span class="dv">4</span>])</a></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/02_combinatorix_rfSecond-1.png" alt="**Figure 2** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the Random Forest Classifier when trained with 200 trees and 50 selected genes. T: T-ALL, Bt: TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid." width="672" />
<p class="caption">
<strong>Figure 2</strong> shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the Random Forest Classifier when trained with 200 trees and 50 selected genes. T: T-ALL, Bt: TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid.
</p>
</div>
<p>Figures 1 and 2 display a heatmap of the confusion table of the classification together with statistics about the accuracy on the table on the right. The confusion table maps the predicted class to the actual class. Consequently, the diagonal of the table is representing correct classifications. In each field of the table, one can see the absolute and relative frequency of the prediction of a certain subtype. Values outside of the diagonal represent a wrong prediction. In the case of Figure 1, the classifier predicted four patients to have the subtype TEL-AML1, whereas they had the type pre-B ALL in reality. Since we are dealing with a multi-classification problem, False Positives and False Negatives are the same for the overall model. However, I will discuss these statistics per class at the end of the section for two selected classifiers. The table on the right displays statistics of the accuracy. The accuracy is simply the percentage of correctly classified samples. Cohen’s Kappa is a different measure of accuracy that accounts for the fact that correct classifications could occur by chance.</p>
<p>The second method of interest in this work was the SVM for classification. Again I start with an overview and discuss more detail later on. For the SVM I used four different kernel methods, namely radial, linear, polynomial and sigmoid. As done with the Random Forest Classifier I used one time all genes and one time 50 genes, selected via the Random Forest variable Importance, as described above. Table 4 shows an overview of the MER, accuracy and its 95% confidence interval of the SVM Classifier with different kernels.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1">svmResultTable &lt;-<span class="st"> </span>resultTable[<span class="dv">7</span><span class="op">:</span><span class="dv">14</span>, ]</a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="kw">kable</span>(svmResultTable, <span class="dt">caption =</span> <span class="st">&quot;Table 4: Overview of the SVM Classifier Performance with different Parameters&quot;</span>)</a></code></pre></div>
<table>
<caption>Table 4: Overview of the SVM Classifier Performance with different Parameters</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">MER</th>
<th align="right">Accuracy</th>
<th align="left">95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SVM Radial Kernel all Genes</td>
<td align="right">0.0599</td>
<td align="right">0.9401</td>
<td align="left">0.9041-0.9761</td>
</tr>
<tr class="even">
<td>SVM Linear Kernel all Genes</td>
<td align="right">0.0719</td>
<td align="right">0.9281</td>
<td align="left">0.889-0.9673</td>
</tr>
<tr class="odd">
<td>SVM Ploynomial Kernel all Genes</td>
<td align="right">0.0719</td>
<td align="right">0.9281</td>
<td align="left">0.889-0.9673</td>
</tr>
<tr class="even">
<td>SVM Sigmoid Kernel all Genes</td>
<td align="right">0.0838</td>
<td align="right">0.9162</td>
<td align="left">0.8741-0.9582</td>
</tr>
<tr class="odd">
<td>SVM Radial Kernel Feature Selection</td>
<td align="right">0.0838</td>
<td align="right">0.9162</td>
<td align="left">0.8741-0.9582</td>
</tr>
<tr class="even">
<td>SVM Linear Kernel Feature Selection</td>
<td align="right">0.0958</td>
<td align="right">0.9042</td>
<td align="left">0.8596-0.9488</td>
</tr>
<tr class="odd">
<td>SVM Ploynomial Kernel Feature Selection</td>
<td align="right">0.1078</td>
<td align="right">0.8922</td>
<td align="left">0.8452-0.9392</td>
</tr>
<tr class="even">
<td>SVM Sigmoid Kernel Feature Selection</td>
<td align="right">0.1377</td>
<td align="right">0.8623</td>
<td align="left">0.81-0.9145</td>
</tr>
</tbody>
</table>
<p>In general, the accuracy of the SVM Classifiers is high ranging from 0.86 to 0.94. The confidence intervals are also reasonably narrow. The overall difference between the accuracy of the classifier with different Kernels is small (under two and a half percentage points), when trained with all genes. Training the SVM Classifier with 50 selected genes yields in a difference of over five percentage points between different kernels. The predictions were slightly more accurate when training the SVM with all genes. I would have expected this since the SVM is highly robust against overfitting since it allows misclassification with the incorporation of so-called slack variables. The ability to always map the feature space also helps to deal with high dimensional data.</p>
<p>Given the small differences in accuracy between most of the kernels, it is hard to argue, why one kernel might be one percentage point more accurate than another. However, I try to compare shortly the most accurate kernel (radial) with the least accurate kernel (sigmoid). Figure 3 shows the heatmap of the confusion table of the SVM Classifier with the radial kernel when trained with all genes. Figure 4 shows the same plot for the SVM Classifier with the sigmoid kernel when trained with 50 selected genes.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">plotResults</span>(resultVector[[<span class="dv">7</span>]], response, <span class="kw">names</span>(resultVector)[<span class="dv">7</span>])</a></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/02_combinatorix_displaySVM-1.png" alt="**Figure 3*** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the SVM Classifier, when trained with the radial Kernel and all. Notes: T: T-ALL, Bt:TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid." width="672" />
<p class="caption">
<strong>Figure 3</strong>* shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the SVM Classifier, when trained with the radial Kernel and all. Notes: T: T-ALL, Bt:TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid.
</p>
</div>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">plotResults</span>(resultVector[[<span class="dv">14</span>]], response, <span class="kw">names</span>(resultVector)[<span class="dv">14</span>])</a></code></pre></div>
<div class="figure" style="text-align: center">
<img src="figures/02_combinatorix_displaySVM2-1.png" alt="**Figure 4** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the SVM Classifier, when trained with the sigmoid Kernel and all. Notes: T: T-ALL, Bt:TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid." width="672" />
<p class="caption">
<strong>Figure 4</strong> shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the SVM Classifier, when trained with the sigmoid Kernel and all. Notes: T: T-ALL, Bt:TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid.
</p>
</div>
<p>Figures 3 and 4 follow the same principle as Figures 1 and 2, thus I do not explain the general structure of the figures a second time. When trained with all genes and the radial kernels the Classifiers predicted all T-ALL types correctly. Four samples with an actual type of TEL-ALM1 were wrongly classified as pre-B ALL subtype. Two samples with the pre-B ALL subtype were falsely classified as hyperdiploid and four hyperdiploid subtypes were wrongly classified as pre-B ALL type. This is actually the exact same distribution as shown in Figure 1.</p>
<p>The total numbers of misclassifications were higher for the SVM Classifier with the sigmoid kernels, but the origins of the mistakes were similar. The classifier made the fewest mistakes for the T-ALL subtypes. The exact numbers can be found in the plot. As stated previously the notions of false positive and false negatives only make sense when looking at the statistics per subtype. This is done in the following for the two best classifiers (best SVM and best Random Forest).</p>
<p>Tables 5 shows the statistics per subtype for the SVM Classifier with all genes and the radial kernel and the Random Forest Classifier with 1000 trees and all genes. The statistics for both classifiers were equal so this table serves as a summary for both classifiers.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">cm.svmBest &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">confusionMatrix</span>(response, resultVector[[<span class="dv">7</span>]])<span class="op">$</span>byClass)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="kw">kable</span>(<span class="kw">round</span>(cm.svmBest,<span class="dv">4</span>), <span class="dt">caption =</span> <span class="st">&quot;Table 5: Classwise performance of the SVM Classifier when trained with the radial kernel and all genes&quot;</span>)</a></code></pre></div>
<table>
<caption>Table 5: Classwise performance of the SVM Classifier when trained with the radial kernel and all genes</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">Class: T</th>
<th align="right">Class: Bt</th>
<th align="right">Class: Bh</th>
<th align="right">Class: Bo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sensitivity</td>
<td align="right">1.0000</td>
<td align="right">0.9149</td>
<td align="right">0.9130</td>
<td align="right">0.9474</td>
</tr>
<tr class="even">
<td>Specificity</td>
<td align="right">1.0000</td>
<td align="right">1.0000</td>
<td align="right">0.9835</td>
<td align="right">0.9380</td>
</tr>
<tr class="odd">
<td>Pos Pred Value</td>
<td align="right">1.0000</td>
<td align="right">1.0000</td>
<td align="right">0.9545</td>
<td align="right">0.8182</td>
</tr>
<tr class="even">
<td>Neg Pred Value</td>
<td align="right">1.0000</td>
<td align="right">0.9677</td>
<td align="right">0.9675</td>
<td align="right">0.9837</td>
</tr>
<tr class="odd">
<td>Precision</td>
<td align="right">1.0000</td>
<td align="right">1.0000</td>
<td align="right">0.9545</td>
<td align="right">0.8182</td>
</tr>
<tr class="even">
<td>Recall</td>
<td align="right">1.0000</td>
<td align="right">0.9149</td>
<td align="right">0.9130</td>
<td align="right">0.9474</td>
</tr>
<tr class="odd">
<td>F1</td>
<td align="right">1.0000</td>
<td align="right">0.9556</td>
<td align="right">0.9333</td>
<td align="right">0.8780</td>
</tr>
<tr class="even">
<td>Prevalence</td>
<td align="right">0.2156</td>
<td align="right">0.2814</td>
<td align="right">0.2754</td>
<td align="right">0.2275</td>
</tr>
<tr class="odd">
<td>Detection Rate</td>
<td align="right">0.2156</td>
<td align="right">0.2575</td>
<td align="right">0.2515</td>
<td align="right">0.2156</td>
</tr>
<tr class="even">
<td>Detection Prevalence</td>
<td align="right">0.2156</td>
<td align="right">0.2575</td>
<td align="right">0.2635</td>
<td align="right">0.2635</td>
</tr>
<tr class="odd">
<td>Balanced Accuracy</td>
<td align="right">1.0000</td>
<td align="right">0.9574</td>
<td align="right">0.9483</td>
<td align="right">0.9427</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co">#grid.arrange(tableGrob(round(cm.svmBest,4)))</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">cm.rfBest &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">confusionMatrix</span>(response, resultVector[[<span class="dv">1</span>]])<span class="op">$</span>byClass)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3"><span class="kw">kable</span>(<span class="kw">round</span>(cm.rfBest, <span class="dv">4</span>), <span class="dt">caption =</span> <span class="st">&quot;Table 6: Classwise performance of the Random Forest Classifier when trainied with 1000 trees and all genes&quot;</span>)</a></code></pre></div>
<table>
<caption>Table 6: Classwise performance of the Random Forest Classifier when trainied with 1000 trees and all genes</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">Class: T</th>
<th align="right">Class: Bt</th>
<th align="right">Class: Bh</th>
<th align="right">Class: Bo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sensitivity</td>
<td align="right">1.0000</td>
<td align="right">0.9149</td>
<td align="right">0.9130</td>
<td align="right">0.9474</td>
</tr>
<tr class="even">
<td>Specificity</td>
<td align="right">1.0000</td>
<td align="right">1.0000</td>
<td align="right">0.9835</td>
<td align="right">0.9380</td>
</tr>
<tr class="odd">
<td>Pos Pred Value</td>
<td align="right">1.0000</td>
<td align="right">1.0000</td>
<td align="right">0.9545</td>
<td align="right">0.8182</td>
</tr>
<tr class="even">
<td>Neg Pred Value</td>
<td align="right">1.0000</td>
<td align="right">0.9677</td>
<td align="right">0.9675</td>
<td align="right">0.9837</td>
</tr>
<tr class="odd">
<td>Precision</td>
<td align="right">1.0000</td>
<td align="right">1.0000</td>
<td align="right">0.9545</td>
<td align="right">0.8182</td>
</tr>
<tr class="even">
<td>Recall</td>
<td align="right">1.0000</td>
<td align="right">0.9149</td>
<td align="right">0.9130</td>
<td align="right">0.9474</td>
</tr>
<tr class="odd">
<td>F1</td>
<td align="right">1.0000</td>
<td align="right">0.9556</td>
<td align="right">0.9333</td>
<td align="right">0.8780</td>
</tr>
<tr class="even">
<td>Prevalence</td>
<td align="right">0.2156</td>
<td align="right">0.2814</td>
<td align="right">0.2754</td>
<td align="right">0.2275</td>
</tr>
<tr class="odd">
<td>Detection Rate</td>
<td align="right">0.2156</td>
<td align="right">0.2575</td>
<td align="right">0.2515</td>
<td align="right">0.2156</td>
</tr>
<tr class="even">
<td>Detection Prevalence</td>
<td align="right">0.2156</td>
<td align="right">0.2575</td>
<td align="right">0.2635</td>
<td align="right">0.2635</td>
</tr>
<tr class="odd">
<td>Balanced Accuracy</td>
<td align="right">1.0000</td>
<td align="right">0.9574</td>
<td align="right">0.9483</td>
<td align="right">0.9427</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="co">#grid.arrange(tableGrob(round(cm.rfBest, 4)),</span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="co">#             tableGrob(round(cm.svmBest,4)), textGrob(&quot;A&quot;), textGrob(&quot;B&quot;),</span></a>
<a class="sourceLine" id="cb15-3" data-line-number="3"> <span class="co">#            nrow = 2, ncol = 2)</span></a></code></pre></div>
<p>lower sensitivity means that there are less true positives, so out of the samples classified with this type, not all samples would actually have this subtype. The specificity is also lower for the SVM for the hyperdiploid subtype. This is also true for the positive predictive value.</p>
<p>The specificity gives the true negative rate, so how many samples really do not belong do a subtype, when not classified as a subtype. So a low specificity will lead to unnecessary treatments and potential psychological distress for a patient, whereas low sensitivity classifiers will fail to identify a disease and thus a patient might not receive treatment.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>In summary, I did not find large differences in the prediction accuracy of the tested classifiers depending on the used parameters. Both the SVM and Random Forest Classifiers are robust against overfitting and thus did not benefit from feature selection. For the Random Forest, 500 trees could be a good number of trees to use and the radial kernels for the SVM classifier performed well in my analysis. Again, the differences in accuracy were not very high.</p>
<p>The parameters could have a bigger impact depending on the dataset. Therefore, it could be interesting for future studies to compare the performance of a Classifier depending on its parameters in different datasets.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1). Springer: 5–32.</p>
</div>
<div id="ref-den2009subtype">
<p>Den Boer, Monique L, Marjon van Slegtenhorst, Renée X De Menezes, Meyling H Cheok, Jessica GCAM Buijs-Gladdines, Susan TCJM Peters, Laura JCM Van Zutven, et al. 2009. “A Subtype of Childhood Acute Lymphoblastic Leukaemia with Poor Treatment Outcome: A Genome-Wide Classification Study.” <em>The Lancet Oncology</em> 10 (2). Elsevier: 125–34.</p>
</div>
<div id="ref-diaz2006gene">
<p>Dı'az-Uriarte, Ramón, and Sara Alvarez De Andres. 2006. “Gene Selection and Classification of Microarray Data Using Random Forest.” <em>BMC Bioinformatics</em> 7 (1). Springer: 3.</p>
</div>
<div id="ref-es">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. 2nd ed. Springer. <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">http://www-stat.stanford.edu/~tibs/ElemStatLearn/</a>.</p>
</div>
<div id="ref-hofmann2006support">
<p>Hofmann, Martin. 2006. “Support Vector Machines-Kernels and the Kernel Trick.” <em>Notes</em> 26 (3).</p>
</div>
<div id="ref-johnstone2009statistical">
<p>Johnstone, Iain M, and D Michael Titterington. 2009. “Statistical Challenges of High-Dimensional Data.” The Royal Society Publishing.</p>
</div>
<div id="ref-smallnlargep">
<p>Kosorok, Michael R, Shuangge Ma, and others. 2007. “Marginal Asymptotics for the ‘Large P, Small N’ Paradigm: With Applications to Microarray Data.” <em>The Annals of Statistics</em> 35 (4). Institute of Mathematical Statistics: 1456–86.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
