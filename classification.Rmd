---
title: "The Influence of Parameter Selection on Supervised Classification"
author: "Maximilian Joas"
date: "`r Sys.Date()`"
bibliography: "lib.bib"
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    highlight: zenburn
    self_contained: no
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  ioslides_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    smaller: yes
    toc: yes
    widescreen: yes
  beamer_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    incremental: no
    keep_tex: no
    slide_level: 2
    theme: Montpellier
    toc: yes
  word_document:
    toc: yes
    toc_depth: '3'
  slidy_presentation:
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    incremental: no
    keep_md: yes
    smaller: yes
    theme: cerulean
    toc: yes
    widescreen: yes
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
font-import: http://fonts.googleapis.com/css?family=Risque
subtitle: "Comparing the Accuracy of the Random Forest Classifier and the SVM Classifier when Trained with different Parameters"
font-family: Garamond
transition: linear
editor_options:
  chunk_output_type: console
---

```{r knitr_settings, include=FALSE, echo=FALSE, eval=TRUE}
library(knitr)
options(width = 300)
knitr::opts_chunk$set(
  fig.width = 7, fig.height = 5,
  fig.path = 'figures/02_combinatorix_',
  fig.align = "center",
  size = "tiny",
  echo = TRUE, eval = TRUE,
  warning = FALSE, message = FALSE,
  results = TRUE, comment = "")


```



# Background
This report is based on a 2009 study by Den Boer et al. [-@den2009subtype]. The goal of this study was to improve the prognostic classification of genetic subtypes of acute lymphoblastic leukemia (ALL) in children. The knowledge of these subtypes facilitate risk stratification and can thus be used to choose the appropriate treatment option.

In this report, however, I use the  Den Boer data set to methodically explore which methods for classification work best and how different parameters influence the classification. Precisely I use support vector machine (SVM) and random forest for classification. Additionally, I use a random forest for feature selection. The aim of this report is to investigate the influence of the used kernel in the SVM Classifier and the influence of the number of trees in the Random Forest Classifier. Additionally, I investigate the influence of feature selection on these two classifiers.

# Methods and Data
## Data 
The used data consists of a data table that stores information about gene expression. Gene expression was measured with microarrays. The data has been already pre-processed before I used it. This included: filtering of weakly expressed genes,
log2 transformation to normalize the raw measurements and
intersample standardization. Additionally, I worked with a data table that stored the metadata of the samples belonging to the gene expression values. In our case, the information of interest is the class of the subtype of acute lymphoblastic leukemia. Table 1 shows the frequency of the different subtypes before filtering (filtering will be done in a later step).


```{r echo=FALSE}
read_chunk('./classification.R')
```
```{r loading, eval=TRUE}
```

## Methods Theory
Due to the limited scope of this work, I will not be able to provide an in-depth description of the methods.
The aim is to only give a very short overview, so the reader understands the key terms of the methods.

Random forest is a widely used method for classification [@diaz2006gene]. The base of a random forest model
are decision trees. A decision tree consists of multiple nodes. At each node the tree is split into
branches depending on the value of an independent variable [@es]. In our case, this would be the expression level
of a particular gene. Which variable is used at a given split is random. However, not all variables are available to sample from at each split. The number of variables available for splitting at each tree node is referred to as the mtry parameter. The end of a branch, the leaf, does not split anymore and represents the decision for a particular class. In our case that would be a  leukemia subtype.
A random forest consists of multiple decision trees. Each tree gives a vote for the final decision of
the class. The class with the majority vote across all trees is the final class of a random forest classification
This number of trees used in a random forest is on interest in this report. I will investigate the
influence of the number of trees on the classification accuracy. 

SVM is another well-established method for classification[@es]. It tries to separate the data by a hyperplane.
The goal is that data points in different classes are as far away from the hyperplane as possible. The
data points nearest to the hyperplane are called support vectors [@es]. Often times the data points are not separable in their original dimension. Nevertheless, it is always possible to separate the data in a higher dimension [@hofmann2006support]. Therefore, we need a way to map the data points to a higher dimension. The function used for this process is called kernel function. There are different types of kernels and I will investigate
the influence of the kernel function on the classification accuracy.

In order to validate the results, I split my data into a training and validation data set. Therefore II implemented leave-one-out-cross -validation (loocv, see section "Methods Implementation"). The evaluation criteria were the misclassification error (MER) respectively the Accuracy of the predictions. Since I was dealing with a multiclass classification task, looking at specificity and sensitivity is not too suitable. This is due to the fact that true positive and true negative counts are the same since a true positive for one class is also a true negative for all the other classes, vice versa for false positives and false negatives.

In microarray studies, it is common to have a high number of measurements in comparison to the number of
samples[@smallnlargep]. This is called a small n large p data set and can lead to overfitting [@johnstone2009statistical]. Therefore, it can be beneficial to select only a subset of independent variables. Ideally, the ones that contain the most information. In order to find these variables I also used the random forest for feature selection. Precisely,
I used the impurity variable importance of the random forest. Before growing the random forest to determine the number of features to select, I tuned the mtry parameter. One note on parameter tuning: It is common to tune the mtry parameter and the number of trees based on the out of bag error of the Random Forest. I did not include tuning for my actual selection of genes and the training. I did not do that because I used leave-one-out-cross-validation, which would have made the process even more computationally intensive. Since the aim was to compare the parameters anyway and not to tune them this is a reasonable approach to do. Since I used the random forest also as classifier I also wanted to include a method for feature selection that is independent of the classification method. Therefore I chose a simple approach: I selected the genes with the most variance across the all samples.



## Methods Implementation
Now that I covered the theoretical basics to understand this report, it is time to present the practical
implementation of the above-described methods. In order to test the classification accuracy, I need to
split the data into a training data set and a validation data set. In order to have as many training and test patients as possible, I decided to use leave-one-out-cross-validation (LOOCV) as an approach. The standard way to do this would be to use the caret packages. I decided,
however, to implement the functionality myself. The reason for this is that I also wanted to use
the LOOCV approach for feature selection to avoid any bias. With my own implementation, I could be sure
that I used the exact same method for feature selection and the training of the model. For my Implementation, I used the foreach package for parallelization, which is highly optimized. Thus,
my implementation is also highly performant. The logic of my function is structured as follows:
I have a base function that performs the actual task, e.g feature selection or classification and another function that implements the LOOCV logic that calls the base function in each iteration.
This makes the code easy to extend since I  only need to write a new base function for a new classification method and can use my existing function for the LOOCV logic.
The base function for classification takes the parameters for the corresponding method, e.g. the number of trees for the random forest and the Kernel method for SVM.

For the feature selection, I applied the same principles I had one function that did the actual feature selection and one function that implements the LOOCV logic and calls the feature selection function. It is also possible to specify how many features should be selected. However, I did not want to choose the number of features arbitrary. Consequently, I sorted the variable importance of all covariates and plotted it. Subsequently, I smoothed the plot in order to make it differentiable. The goal was to find the point where the variable importance drops the steepest. This point I used as a cutoff for the number of features that I selected. In order to find the point, I took the highest value of the second derivative
of the smoothed line. This process is a highly reusable and automated process to find the number of variables to select. The same was done when I used the variance as feature selection criteria. Furthermore, I wrote some functions to display and plot the results.




# Results and Interpretation
The result section is structured as follows: Firstly, I present the results of the feature selection process. Secondly, I will give an overview of the results of the random forest classifier for different numbers of trees. Subsequently, I will present the most meaningful results more in-depth. I will do the same for the SVM classifier afterward. Finally, I will compare the best classifiers across the two methods. Each description of a Table / Figure is followed by a short interpretation.



I included only classes that contained more than 30 patients. After the filtering, four classes remained. Figure 1 gives an overview of the remaining classes and how many cases each class contains.

```{r filter, eval=TRUE, fig.width=1.5, fig.height=1.5, fig.cap="Figure 1: Remaining Classes after Class Filtering. T: T-cell acute lymphoblastic leukaemia, Bo: Acute B Lymphoblastic Leukemia, Bt: TEL-AML1 , Bh: hyperdiploid childhood acute lymphoblastic leukemia"}
```

In order to decide how many genes to select I plotted the variable importance of the 200 most important genes according to the random forest variable importance. Figure 2 shows this plot together with a smoothed line of the variable importance and the point where the variable importance drops the steepest (according to the smoothed line).

```{r prepareSelect, eval=TRUE, fig.cap="**Figure 1**: Plot of the development of the Random Forest variable importance of the 200 most important genes. For practical reasons concerning the smoothing, I limited the number of genes to 200. The red dots show the actual value of the variable importance, the green line smothes these values and the blue vertical line indicated the point where the smoothed line changes the most."}
```
Accordingly to the plot, I chose to use 50 genes, when performing the feature selection.
For the comparison of the classifiers, I used two sets of selected genes: One set where I selected 50 genes and one set where I included all genes. Of course, there are many other ways to determine how many features should be included. The advantage of my approach is that it is reusable and that it quantifies the loss of importance of the features. The drawbacks are that it does not account for the total number of features in the data set and the selected number can be unfavorable in extreme cases of the distribution of the variable importance i.e. it selects only one or all genes. However, it was not the focus of this report to find a method that optimally selects the number of features. Thus, I wanted just an automated, out of the box way to do this. Figure 2 shows the same plot for the variance. Here I chose 125 genes.

```{r plotVariance, eval=TRUE, fig.cap="**Figure 2**: Plot of the variance of the 500 genes with the highest variances across samples. For practical reasons concerning the smoothing, I limited the number of genes to 500. The red dots show the actual value of the variable importance, the green line smothes these values and the blue vertical line indicated the point where the smoothed line changes the most."}
```
```{r select, eval=TRUE}
```

For the actual comparison of the Random Forest Classifiers, I used the following numbers of trees: 200, 500 and 1000. Table 3 shows an overview of the MER, Accuracy and 95% confidence interval.

 ```{r execute, eval=TRUE}
```
```{r overviewResultsRF, eval=TRUE}
```



The accuracy when using all genes is with over 94% very high. The 95% confidence interval is also reasonably narrow, which gives security about the result. An accuracy of 94% means that 94% of the samples were classified correctly.
The number of trees seems to have little to no influence on the prediction accuracy when using all genes. Contemplating the prediction accuracy when the classifier was trained with the 50 selected genes by the Random Forest Variable Importance, the number of trees still has no meaningful impact. The accuracy ranges from 90% to 93% and is with that slightly lower than when trained with all genes. In contrast when the Classifier was trained with with the genes selected by the variance method the results were strange. The Random Forest with 500 trees did not classify anything correct. I regard this as an outlier, but in general that makes the Classifier highly questionable. Especially in the light that the prediction accuracy was high when trained with 200 genes (87 %) and 1000 genes (95 %). So here the increase of the number of trees lead to an increase in the accuracy.

It is a bit unexpected that the number of trees did not have a meaningful influence on most of the prediction accuracies. The used numbers of trees were all rather high and thus could be already considered as nearly optimal. The reason why I used many trees, is that the data set does contain over 20,000 genes. So using fewer trees could have lead to more differences in the prediction accuracies. The fact that feature selection did not have a meaningful impact on the Classifier is not unexpected. Firstly, Random Forests can deal with a large number of covariates [@breiman2001random]. This is due to the fact that it uses only a subset of covariates at each split. Secondly, I have already used a Random Forest to select covariates. The one thing that stands out is that the one parametrization of the classifier did not classify anything correct. This makes the classifier highly unreliable.

In the following, I will report on the detailed results of the Random Forest Classifier with 1000 trees and all genes (which yields the exact same results as the other Random Forest Classifiers expect one). Additionally, I report on the results of the Random Forest Classifier when trained with 125 genes (selected with the variance method) and 200 trees.


```{r displayResult,eval=TRUE, fig.cap="**Figure 3** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the Random Forest Classifier when trained with 1000 trees and all genes. Notes: T: T-ALL, Bt: TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid."}
```

```{r rfSecond, eval=TRUE, fig.cap="**Figure 4** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the Random Forest Classifier when trained with 200 trees and 50 selected genes. T: T-ALL, Bt: TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid."}
```


Figures 3 and 4 display a heatmap of the confusion table of the classification together with statistics about the accuracy on the table on the right. The confusion table maps the predicted class to the actual class. Consequently, the diagonal of the table is representing correct classifications. In each field of the table, one can see the absolute and relative frequency of the prediction of a certain subtype. Values outside of the diagonal represent a wrong prediction. In the case of Figure 1, the classifier predicted three patients to have the subtype TEL-AML1, whereas they had the type pre-B ALL in reality. Since we are dealing with a multi-classification problem, False Positives and False Negatives are the same for the overall model. However, I will discuss these statistics per class at the end of the section for two selected classifiers. The table on the right displays accuracy statistics. The accuracy is simply the percentage of correctly classified samples. Cohen's Kappa is a different measure of accuracy that accounts for the fact that correct classifications could occur by chance.

The second method of interest in this work was the SVM for classification. Again I start with an overview and discuss more details later on. For the SVM I used four different kernel methods, namely radial, linear, polynomial and sigmoid. As done with the Random Forest Classifier I used one time all genes and one time 50 genes, selected via the Random Forest variable Importance, as described above. Table 4 shows an overview of the MER, accuracy and its 95% confidence interval of the SVM Classifier with different kernels.


```{r overviewResultsSVM, eval=TRUE}
```


In general, the accuracy of the SVM Classifiers is heterogenous varying from 0.86 to 0.95, with to not so accurate version with an accuracy of 41 %, 49 % and 73 %. The confidence intervals are also reasonably narrow.
The overall difference between the accuracy of the classifier with different Kernels is heterogenous (49 % - 95 %) when trained with all genes. The polynomial kernel yielded the best accuracy, the radial the worst for this case. When using only the selected genes by the Random Forest the accuracy becomes more homogenous ranging from 86 % to 95 %, with this time the radial kernel being the best. The selected genes with the variance method lead again to heterogeneous results (41 % - 91 %).


It seems that different kernels are highly sensitive to the type of selected genes. The number of selected genes seems to have no real influence. Overall the polynomial kernel seems to be a good method to map the data to a higher dimension. It could be because it gives takes combinations of features into account. In general we want to have a kernel that is not too dependent on the selected genes, so the polynomial kernel what be the prefered choice here.
Figure 5 shows the heatmap of the confusion table of the SVM Classifier with the polynomial kernel when trained with all genes. Figure 6 shows the same plot for the SVM Classifier with the sigmoid kernel when trained with 50 selected genes.



```{r displaySVM, eval=TRUE, fig.cap="**Figure 5** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the SVM Classifier when trained with the polynomial and all genes. Notes: T: T-ALL, Bt:TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid."}
```



```{r displaySVM2, eval=TRUE, fig.cap="**Figure 6** shows a heatmap of the confusion table on the left and accuracy statistics on the right. This particular data comes from the SVM Classifier when trained with the radial kernel and the 125 genes from the variance method. Notes: T: T-ALL, Bt:TEL-AML1, Bo: pre-B ALL, Bh: hyperdiploid."}
```

Figures 5 and 6 follow the same principle as Figures 3 and 4, thus I do not explain the general structure of the figures a second time. When trained with all genes and the polynomial kernel the tlassifier predicted all T-ALL types correctly. Three samples with an actual type of TEL-ALM1 were wrongly classified as pre-B ALL subtype. Two samples with the pre-B ALL subtype were falsely classified as hyperdiploid and one as TEL-AML1. Two hyperdiploid subtypes were wrongly classified as pre-B ALL type. 

The distribution of false classification when trained with the radial kernel and the genes selected with the variance method was rather odd. The classifier predicted 78 % of all subtypes to be the pre-B ALL type. This false positive rate for the pre-B ALL subtype leads consequently to a low false negative and low true positive rate for the other classes. 
As stated previously the notions of false positive and false negatives only make sense when looking at the statistics per subtype. This is done in the following for the two best classifiers (best SVM and best Random Forest).


Tables 5 shows the statistics per subtype for the SVM Classifier with all genes and the radial kernel and the Random Forest Classifier with 1000 trees and all genes. The statistics for both classifiers were equal so this table serves as a summary for both classifiers.


```{r inDepth, eval=TRUE}
```
A few  remarks to specificity, sensitivity and the positive predictive value: A high specificity means that a negative test result means that is very likely that a patient does not have a disease. So a high specificity is very desirable when dealing with serious illness, since it has lethal consequences if the test fails to identify a healthy patient. A high sensitivity says that it is very likely that a positive test means that a patient has actual a disease. A high sensitivity thus is especially important for diseases where the treatment is dangerous. In practice the value of the positive predictive value is particularly interesting since it gives the likelihood of actually having a disease when receiving a positive test result. As mentioned before the notion of these criterias are usually used in two class classifications and not common in multiclass classification settings. Thus Table 5 shows these criteria only classwise. Since we contemplating the two best classifiers all values are very high. It is notable however that the T-ALL type gets classified perfectly, while there are few mistakes for the other classes. Perhaps the T-ALL type could have a very distinct expression pattern.
 



# Conclusion
In summary, the different classifiers where dependent on the combination of selected genes and parameters. Different parameter sets yielded in highly heterogenous results depending on which genes I included in the analysis. However, both the SVM and Random Forest Classifiers are robust against overfitting and thus did not benefit from dimension reduction. For the Random Forest, 1000 trees could be a good number of trees to use and the polynomial kernel for the SVM classifier performed well in my analysis. Again, the differences in accuracy were not very high.

The parameters could have a bigger impact (independent of the selected genes) depending on the data set. Therefore, it could be interesting for future studies to compare the performance of a Classifier depending on its parameters in different datasets.

# References








